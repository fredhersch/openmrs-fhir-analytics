{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Analytics on FHIR with Open Health Stack","text":"<p>The Open Health Stack's FHIR Analytics components provide a powerful way to transform complex FHIR (Fast Healthcare Interoperability Resources) data into formats suitable for analysis. </p> <p>Using OHS, developers can use familiar packages and tools to build analytics solutions for different use cases: from generating reports and powering dashboards to facilitating machine learning and exploratory data science initiatives.</p> <p>OHS FHIR Analytics components are designed to help developers to build horizontally scalable analytics solutions for a range of problems on top of FHIR data that can be deployed into on-prem or cloud environments.</p>"},{"location":"#key-features","title":"Key features","text":"<ul> <li> <p>Transform FHIR JSON Resources (STU3 and R4) to Parquet representations based on the SQL-on-FHIR-v1 schema. Support for profiles and extensions</p> </li> <li> <p>Horizontally scalable Apache Beam based ETL pipeline to continuously transform FHIR data into analytics friendly formats. Support for local, on-prem or cloud based runners</p> </li> <li> <p>Seamless support for different Data Warehouses from traditional Relational (such as postgres), OLAP Database Engines (such as duckdb or clickhouse) to Distributed query engines (such as Spark SQL)</p> </li> <li> <p>Define views in SQL or as ViewDefinition Resources and create flat tables to make it easier to query data using common language like simple SQL or python</p> </li> </ul> <p>Next page: Components</p>"},{"location":"getting_started/","title":"Getting Started","text":"<p>In this section we will look at how to get started with FHIR Data Pipes. </p> <p>If you are looking for a quick evaluation or prefer to deploy via docker image, check out the single machine deployment section which uses a pre-configured docker image and instructions for setting up an end-to-end deployment.</p>"},{"location":"getting_started/#set-up-fhir-data-pipes","title":"Set-up FHIR Data Pipes","text":"<p>The pipelines directory contains code to transform data from a FHIR source to either Apache Parquet files for analysis or another FHIR store for data integration.</p> <p>See this page if you are using OpenMRS.</p> <p>Reading data from a FHIR Source</p> <p>The following options are supported for reading the FHIR source data (input data):</p> <ul> <li> <p>FHIR-Search: This mode uses FHIR Search APIs to select resources to     copy, retrieves them as FHIR resources, and transfers the data via FHIR APIs     or Parquet files. This mode should work with any FHIR server or Facade that has an FHIR Search API end-point</p> </li> <li> <p>ndjson: This mode uses ndjson bundles as inputs into the Pipeline. These could be generated using the Bulk Export API or other data transformation methods</p> </li> <li> <p>JDBC: This mode uses the Java Database Connectivity (JDBC)     API to read     FHIR resources directly from the database of a HAPI FHIR server configured to use a PostgreSQL     database     or an OpenMRS instance using MySQL.     Note: JDBC support beyond HAPI FHIR and OpenMRS is not currently     planned. Our long-term approach for a generic high-throughput alternative is to use     the FHIR Bulk Export API.</p> </li> </ul> <p>There are two options for transforming the data (output):</p> <ul> <li>Parquet: Outputs the FHIR resources as Parquet files, using the     SQL-on-FHIR schema.</li> <li>FHIR: Copies the FHIR resources to another FHIR server using FHIR APIs.</li> </ul>"},{"location":"getting_started/#setup","title":"Setup","text":"<p>Clone the FHIR Data Pipes project to your machine. Set the <code>utils</code> directory to world-readable: <code>chmod -R 755 ./utils</code>. Build binaries by running <code>mvn clean install</code> from the root directory of the repository.</p>"},{"location":"getting_started/#run-the-pipeline","title":"Run the pipeline","text":"<p>Run the pipeline directly using the <code>java</code> command:</p> run pipeline<pre><code>java -cp ./pipelines/batch/target/batch-bundled-0.1.0-SNAPSHOT.jar \\\n    org.openmrs.analytics.FhirEtl \\\n    --fhirServerUrl=http://example.org/fhir \\\n    --[see additional parameters below]\n</code></pre> <p>Add the necessary parameters depending on your use case. The methods used for reading the source FHIR server and outputting the data depend on the parameters used. You can output to both Parquet files and a FHIR server by including the required parameters for both.</p>"},{"location":"getting_started/#parameters","title":"Parameters","text":"<p>This section documents the parameters used by the various pipelines. For more information on parameters, see [TO DO: Add page] <code>FhirEtlOptions</code>.</p>"},{"location":"getting_started/#common-parameters","title":"Common parameters","text":"<p>These parameters are used regardless of other pipeline options.</p> <ul> <li><code>fhirServerUrl</code> - The base URL of the source FHIR server. Required.</li> <li><code>fhirServerUserName</code> - The HTTP Basic Auth username to access the FHIR         server APIs. Default: <code>admin</code></li> <li><code>fhirServerPassword</code> - The HTTP Basic Auth password to access the FHIR     server APIs. Default: <code>Admin123</code></li> <li><code>resourceList</code> - A comma-separated list of FHIR     resources to include in the     pipeline. Default: <code>Patient,Encounter,Observation</code></li> <li><code>runner</code> - The Apache Beam     Runner     to use. Pipelines supports DirectRunner and FlinkRunner by default; other     runners can be enabled by Maven profiles, e.g., DataflowRunner. Default: <code>DirectRunner</code></li> </ul>"},{"location":"getting_started/#fhir-search-input-parameters","title":"FHIR-Search input parameters","text":"<p>The pipeline will use FHIR-Search to fetch data as long as <code>jdbcModeEnabled</code> is unset or false.</p> <ul> <li><code>batchSize</code> - The number of resources to fetch in each API call. Default:     <code>100</code></li> </ul>"},{"location":"getting_started/#jdbc-input-parameters","title":"JDBC input parameters","text":"<p>JDBC mode is used if <code>jdbcModeEnabled=true</code>.</p> <p>To use JDBC mode, first create a copy of hapi-postgres-config.json and edit the values to match your database server.</p> <p>Next, include the following parameters:</p> <ul> <li><code>jdbcModeEnabled=true</code></li> <li><code>fhirDatabaseConfigPath=./path/to/config.json</code></li> </ul> <p>If you are using a HAPI FHIR server, also include:</p> <ul> <li><code>jdbcModeHapi=true</code></li> <li><code>jdbcDriverClass=org.postgresql.Driver</code></li> </ul> <p>All JDBC parameters:</p> <ul> <li><code>jdbcModeEnabled</code> - If true, uses JDBC mode. Default: <code>false</code></li> <li><code>fhirDatabaseConfigPath</code> - Path to the FHIR database config for JDBC mode.     Default: <code>../utils/hapi-postgres-config.json</code></li> <li><code>jdbcModeHapi</code> - If true (with <code>jdbcModeEnabled</code>), uses JDBC mode for a HAPI     source server. Default: <code>false</code></li> <li><code>jdbcFetchSize</code> - The fetch size of each JDBC database query. Default:     <code>10000</code></li> <li><code>jdbcMaxPoolSize</code> - The maximum number of database connections. Default:     <code>50</code></li> <li><code>jdbcDriverClass</code> - The JDBC driver to use. Should be set to     <code>org.postgresql.Driver</code> for HAPI FHIR Postgres access. Default:     <code>com.mysql.cj.jdbc.Driver</code></li> </ul>"},{"location":"getting_started/#parquet-output-parameters","title":"Parquet output parameters","text":"<p>Parquet files are output when <code>outputParquetPath</code> is set.</p> <ul> <li><code>outputParquetPath</code> - The file path to write Parquet files to, e.g.,     <code>./tmp/parquet/</code>. Default: empty string, which does not output Parquet files.</li> <li><code>secondsToFlushParquetFiles</code> - The number of seconds to wait before flushing     all Parquet writers with non-empty content to files. Use <code>0</code> to disable.     Default: <code>3600</code>.</li> <li><code>rowGroupSizeForParquetFiles</code> - The approximate size in bytes of the     row-groups in Parquet files. When this size is reached, the content is     flushed to disk. This is not used if there are less than 100 records. Use     <code>0</code> to use the default Parquet row-group size. Default: <code>0</code>.</li> </ul>"},{"location":"getting_started/#fhir-output-parameters","title":"FHIR output parameters","text":"<p>Resources will be copied to the FHIR server specified in <code>fhirSinkPath</code> if that field is set.</p> <ul> <li><code>fhirSinkPath</code> - A base URL to a target FHIR server, or the relative path of     a GCP FHIR store, e.g. <code>http://localhost:8091/fhir</code> for a FHIR server or     <code>projects/PROJECT/locations/LOCATION/datasets/DATASET/fhirStores/FHIR-STORE-NAME</code>     for a GCP FHIR store. If using a GCP FHIR store,     see here for setup information.     default: none, resources are not copied</li> <li><code>sinkUserName</code> - The HTTP Basic Auth username to access the FHIR sink. Not     used for GCP FHIR stores.</li> <li><code>sinkPassword</code> - The HTTP Basic Auth password to access the FHIR sink. Not     used for GCP FHIR stores.</li> </ul>"},{"location":"getting_started/#a-note-about-beam-runners","title":"A note about Beam runners","text":"<p>If the pipeline is run on a single machine (i.e., not on a distributed cluster), for large datasets consider using a production grade runner like Flink. </p> <p>This can be done by adding the parameter <code>--runner=FlinkRunner</code> (use <code>--maxParallelism</code> and <code>--parallelism</code> to control parallelism). This should not give a significant run time improvement but may avoid some of the memory issues of <code>DirectRunner</code>.</p> <p>[To-do: Read more in the \u201cApache Beam Runners\u201d guide]</p>"},{"location":"getting_started/#example-configurations","title":"Example configurations","text":"<p>These examples are set up to work with local test servers.</p>"},{"location":"getting_started/#fhir-search-to-parquet-files","title":"FHIR Search to Parquet files","text":"<p>Example run:</p> <pre><code>java -cp ./pipelines/batch/target/batch-bundled-0.1.0-SNAPSHOT.jar \\\n    org.openmrs.analytics.FhirEtl \\\n    --fhirServerUrl=http://localhost:8091/fhir \\\n    --outputParquetPath=/tmp/TEST/ \\\n    --resourceList=Patient,Encounter,Observation\n</code></pre>"},{"location":"getting_started/#hapi-fhir-jdbc-to-a-fhir-server","title":"HAPI FHIR JDBC to a FHIR server","text":"<p>Example run:</p> <pre><code>java -cp ./pipelines/batch/target/batch-bundled-0.1.0-SNAPSHOT.jar \\\n    org.openmrs.analytics.FhirEtl \\\n    --fhirServerUrl=http://localhost:8091/fhir \\\n    --resourceList=Patient,Encounter,Observation \\\n    --fhirDatabaseConfigPath=./utils/hapi-postgres-config.json \\\n    --jdbcModeEnabled=true --jdbcModeHapi=true \\\n    --jdbcMaxPoolSize=50 --jdbcFetchSize=1000 \\\n    --jdbcDriverClass=org.postgresql.Driver \\\n    --fhirSinkPath=http://localhost:8099/fhir \\\n    --sinkUserName=hapi --sinkPassword=hapi\n</code></pre>"},{"location":"getting_started/#how-to-query-the-data-warehouse","title":"How to query the data warehouse","text":"<p>To query Parquet files, load them into a compatible data engine such as Apache Spark.</p> <p>The single machine Docker Compose configuration runs the pipeline and loads data into an Apache Spark Thrift server for you.</p>"},{"location":"getting_started/#single-machine-deployment","title":"Single Machine Deployment","text":"<p>The easiest way to get started is to set up a [single machine deployment] using a docker image provided in the repository.</p> <p>By default we use the Basic \"Single Machine\"  Docker Compose Configuration. For descriptions of the different available compose configurations see here</p> <p>By default, the Single Machine docker compose configuration will:</p> <ul> <li>Bring up the FHIR Pipelines Controller plus a Spark Thrift Server</li> <li>Start the Web Control Panel for interacting with the Controller for running full and incremental Pipelines and registering new views</li> <li>Generate Parquet files for in the /docker/dwh directory </li> <li>Run the SQL-on-FHIR queries defined in /config/views/*.sql and register these via the Thrift Server</li> <li>Generate flattened views (defined in /config/views/*.json) in the database configured by sinkDbConfigPath (you will need to ensure the database is created)</li> </ul> <p>The rest of this guide covers setting up and deploying an end-to-end environment for running analytics queries on FHIR Data. It includes optional instructions for provisioning a FHIR Server with sample data.</p> <p>To learn how the Pipelines Controller works on its own, go to the FHIR FHIR Pipelines Controller.</p>"},{"location":"getting_started/#requirements","title":"Requirements","text":"<ul> <li>Docker - If you are using Linux, Docker must be in sudoless mode</li> <li>Docker Compose</li> <li>The FHIR Data Pipes repository, cloned onto the host machine</li> </ul>"}]}