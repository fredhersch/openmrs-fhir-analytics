{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Introduction","text":"<p>The Open Health Stack's FHIR Data Pipes provides a collection of tools to transform complex FHIR (Fast Healthcare Interoperability Resources) data into formats for running analytics workloads and building services. </p> <p>Using OHS, developers can use familiar languages (such as python or ANSI SQL), packages and tools to build analytics solutions for different use cases: from generating reports and powering dashboards to exploratory data science and machine learning.</p> <p></p> <p>Caption: FHIR Data Pipes Graphic </p> <p>Key features</p> <ul> <li> <p>Transform FHIR JSON Resources (STU3 and R4) to (near) \"lossless\" Parquet on FHIR representation based on the SQL-on-FHIR-v1 schema</p> </li> <li> <p>Horizontally scalable Apache Beam based ETL pipeline to continuously transform FHIR data for use in downstream applications. Support for local, on-prem or cloud runners and deployment options</p> </li> <li> <p>Seamless support for different Data Warehouses from traditional RDBMS (such as PostgreSQL), OLAP Database Engines (such as duckdb) to Distributed query engines (such as SparkSQL, Trino or PrestoDB)</p> </li> <li> <p>Define views in ANSI SQL or as ViewDefinition Resources to create flattened tables to power different analytics solutions</p> </li> </ul> <p>Usage FHIR Data Pipes</p> <ul> <li> <p>The primary use case for FHIR Data Pipes is to enable continuous transformation of FHIR Data into analytics friendly representations.</p> </li> <li> <p>A secondary use case is for piping FHIR data from different sources into a central FHIR repository (early stage).</p> </li> </ul>"},{"location":"#horizontally-scalable-etl-pipeline","title":"Horizontally scalable ETL Pipeline","text":"<p>The ETL Pipeline is implemented in Apache Beam and can utilize different runners depending on the environment (local, on-prem or cloud). Extraction:</p> <p>FHIR Data Pipes is designed to work with any FHIR source data. </p> <ul> <li>FHIR Server API (e.g. HAPI FHIR)</li> <li>FHIR Facade (e.g. OpenMRS with the FHIR2 omod)</li> <li>FHIR ndjson (e.g from bulk export API) </li> </ul>"},{"location":"#transformation-to-near-lossless-schema-as-parquet-files","title":"Transformation to near \"lossless\" schema as Parquet files","text":"<ul> <li>Uses bunsen to transform from FHIR Resources (STU3, R4) to the SQL-on-FHIR-v1 schema on a per Resource basis</li> <li>Near \"lossless\" transformation; with the exception of limits based on setting recursive depth (e.g. for nested QuestionnaireResponses) </li> <li>Configurable support for FHIR profiles and extensions on a per Resource basis</li> </ul>"},{"location":"#loading-into-dwh","title":"Loading into DWH","text":"<p>FHIR Data Pipes supports different SQL Data Warehouse options depending on the needs of the project. These include:</p> <ul> <li>Loading Parquet into an OLAP data warehouse such as SparkSQL, duckdb or others </li> <li>Distributed query engine such as SparkSQL (example provided), Trino or PrestoDB</li> <li>Traditional RDBMS such as Postgres or MySQL (when applying FHIR ViewDefinitions)</li> </ul>"},{"location":"#querying-data","title":"Querying Data","text":"<p>When working with the SQL-on-FHIR-v1 schema, one major challenge is handling the many nested or repeated fields (such as condeable concepts). </p> <p>This depends on the different nuances of SQL dialects. For example:</p> <ul> <li>Spark SQL: <code>LATERL VIEW explode()</code></li> <li>BigQuery: <code>Simple JOIN</code></li> <li>JSON: <code>CROSS JOIN LATERAL jsonb_array_elements()</code></li> </ul> <p>[Click here to see examples of SQL-on-FHIR-v1 queries]</p> <p>Example of an SQL-on-FHIR-v1 query using the EXPLODE and LATERAL VIEW approach in Spark SQL</p> <pre><code>SELECT\n  O.id AS obs_id, O.subject.PatientId AS patient_id,\n  O.status AS status\nFROM Observation AS O LATERAL VIEW explode(code.coding)\n  AS OCC LATERAL VIEW\n  explode(O.value.codeableConcept.coding) AS OVCC\nWHERE OCC.`system`=\n    \"https://openconceptlab.org/orgs/CIEL/sources/CIEL\"\n  AND OCC.code=\"1255\"\n  AND OVCC.`system`=\n    \"https://openconceptlab.org/orgs/CIEL/sources/CIEL\"\n  AND OVCC.code=\"1256\"\n  AND YEAR(O.effective.dateTime) = 2010;\n</code></pre> <p>A common pattern to address this is to flatten the data into a set of views of tables which can then be queried using simpler SQL statements. </p>"},{"location":"#flattening-fhir-resources","title":"Flattening FHIR Resources","text":"<p>To make it easier to query data, FHIR Data Pipes provides two approaches for flattening the FHIR Resources into virtual or materialized views</p> <ol> <li>SQL queries to generate virtual views (outside the pipeline)</li> <li>Apply FHIR ViewDefinition Resources (SQL-on-FHIR-v2 spec) to generated materialized views (within the pipeline)</li> </ol> <p>For both of these approaches, predefined \u201cview definitions\u201d (SQL or JSON) for common resources are provided. These can be modified or extended.</p>"},{"location":"#sql-defined-views","title":"SQL defined views","text":"<p>These are samples of more complex SQL-on-FHIR queries for defining flat views for common FHIR Resources (see here for up to date list)</p> <p>For each Resource supported, a \u201c_flat\u201d view is generated (e.g. Patient_flat, Observation_flat). These can be used in the downstream SQL query engine. The queries, which have .sql suffix can be found here: /docker/config/views (e.g Patient_flat.sql)</p> <p>An example of a flat view for the Observation Resource is below</p> <pre><code>CREATE OR REPLACE VIEW flat_observation AS\nSELECT O.id AS obs_id, O.subject.PatientId AS patient_id,\n        OCC.`system` AS code_sys, OCC.code,\n        O.value.quantity.value AS val_quantity,\n        OVCC.code AS val_code, OVCC.`system` AS val_sys,\n        O.effective.dateTime AS obs_date\n      FROM Observation AS O LATERAL VIEW OUTER explode(code.coding) AS OCC\n        LATERAL VIEW OUTER explode(O.value.codeableConcept.coding) AS OVCC\n</code></pre> <p>Learn more:</p> <p>Examples of SQL-on-FHIR-v1 queries for defining views </p>"},{"location":"#fhir-viewdefinition-resource-views","title":"FHIR ViewDefinition Resource views","text":"<p>The SQL-on-FHIR-v2 specification defines a standards based pattern for defining Views as FHIRPath expressions in a logical structure to specify the column names and values (as unnested items).</p> <p>A system (pipeline or library) that implements the \u201cView Layer\u201d of the specification provides a View Runner that is able to process these FHIR ViewDefinition Resources over the \u201cData Layer\u201d (lossless representation of the FHIR data). The output of this are a set of portable, tabular views that can be consumed by the \u201cAnalytics Layer\u201d which is any number of tools that can be used to work with the resulting tabular data.</p> <p>FHIR Data Pipes is a reference implementation of the SQL-on-FHIR-v2 specification:</p> <ul> <li> <p>The \"View Runner\" is by default part of the ETL Pipelines and uses the transformed Parquet files as the \u201cData Layer\u201d. This can be extracted to be a stand-alone component if required</p> </li> <li> <p>When enabled as part of the Pipeline configuration, it will apply the ViewDefinition Resources from the /config/views folder and materialize the resulting tables to the PostgresSQL database.</p> </li> <li> <p>A set of pre-defined ViewDefinitions for common FHIR Resources is provided as part of the default package. These can be adapted, replaced and extended</p> </li> <li> <p>The FHIR Data Pipes provides a simple ViewDefinition Editor which can be used to explore FHIR ViewDefinitions and apply these to individual FHIR Resources</p> </li> </ul>"},{"location":"advanced_/","title":"Local Test Servers","text":"<p>This guide shows how to bring up servers with Docker images to test the FHIR Data Pipes pipelines.</p> <p>There are 3 Docker server configurations you can use for testing:</p> <ul> <li>HAPI FHIR server with Postgres (source)</li> <li>OpenMRS Reference Application with MySQL (source)</li> <li>HAPI FHIR server (destination)</li> </ul>"},{"location":"advanced_/#instructions","title":"Instructions","text":"<p>Note: All commands are run from the root directory of the repository.</p> <ol> <li> <p>Create an external Docker network named <code>cloudbuild</code>:</p> <pre><code>docker network create cloudbuild\n</code></pre> </li> <li> <p>Bring up a FHIR source server for the pipeline. This can be either:</p> <ul> <li> <p>HAPI FHIR server with     Postgres:</p> <pre><code>docker-compose  -f ./docker/hapi-compose.yml up  --force-recreate -d\n</code></pre> <p>The base FHIR URL for this server is <code>http://localhost:8091/fhir</code>. If you get a CORS error when accessing the URL, try manually refreshing (e.g. ctrl-shift-r).</p> </li> <li> <p>OpenMRS Reference Application with     MySQL:</p> <pre><code>docker-compose -f ./docker/openmrs-compose.yml up --force-recreate -d\n</code></pre> <p>The base FHIR URL for this server is <code>http://localhost:8099/openmrs/ws/fhir2/R4</code></p> </li> </ul> </li> <li> <p>Upload the synthetic data stored in     sample_data     to the FHIR server that you brought up using the Synthea data     uploader.</p> <p>The uploader requires the <code>google-auth</code> Python library, which you can install using:</p> <pre><code>pip3 install --upgrade google-auth\n</code></pre> <p>For example, to upload to the HAPI FHIR server brought up in the previous step, run:</p> <pre><code>python3 ./synthea-hiv/uploader/main.py HAPI http://localhost:8091/fhir \\\n--input_dir ./synthea-hiv/sample_data --cores 8\n</code></pre> <p>Depending on your machine, using too many cores may slow down your machine or cause JDBC connection pool errors with the HAPI FHIR server. Reducing the number of cores using the <code>--cores</code> flag should help at the cost of increasing the time to upload the data.</p> </li> <li> <p>(optional) If you only want to output Apache Parquet files, there is no additional     setup. If you want to test outputting to another FHIR server, then bring up a     destination HAPI FHIR     server:</p> <pre><code>docker-compose  -f ./docker/sink-compose.yml up  --force-recreate -d\n</code></pre> <p>The base URL for this server is <code>http://localhost:8098/fhir</code>.</p> </li> </ol>"},{"location":"advanced_/#additional-notes-for-openmrs","title":"Additional notes for OpenMRS","text":"<p>Once running you can access OpenMRS at http://localhost:8099/openmrs/ using username \"admin\" and password \"Admin123\". The Docker image includes the required FHIR2 module and demo data. Edit <code>docker/openmrs-compose.yaml</code> to change the default port.</p> <p>Note: If <code>docker-compose</code> fails, you may need to adjust file permissions. In particular if the permissions on <code>mysqld.cnf</code> is not right, the <code>datadir</code> set in this file will not be read by MySQL and it will cause OpenMRS to require its <code>initialsetup</code> (which is not needed since the MySQL image already has all the data and tables needed):</p> <pre><code>$ docker-compose -f docker/openmrs-compose.yaml down -v\n$ chmod a+r docker/mysql-build/mysqld.cnf\n$ chmod -R a+r ./utils\n$ docker-compose -f docker/openmrs-compose.yaml up\n</code></pre> <p>In order to see the demo data in OpenMRS you must rebuild the search index. In OpenMRS go to Home &gt; System Administration &gt; Advanced Administration. Under Maintenance go to Search Index then Rebuild Search Index.</p>"},{"location":"advanced_guides/","title":"Advanced guides","text":""},{"location":"advanced_guides/#transformation-to-parquet-using-sql-on-fhir-schema","title":"Transformation to parquet using SQL-on-FHIR schema","text":"<ul> <li>Document the SQL-on-FHIR schema:<ul> <li>Recursive depth</li> <li>Contained resources</li> <li>Explain the FHIR to AvroConverter and Bunsen</li> <li>FHIR version support (and future plans)</li> <li>Support for extensions</li> <li>How to use StructureDefinitions \u2192 link to tutorial</li> </ul> </li> </ul>"},{"location":"advanced_guides/#parquet-representation","title":"Parquet representation","text":"<ul> <li>Note about this</li> <li>Horizontal scalability</li> <li>Managing parquet files</li> </ul>"},{"location":"advanced_guides/#pipelines-controller","title":"Pipelines Controller","text":"<p>https://github.com/google/fhir-data-pipes/wiki/Try-out-the-FHIR-Pipelines-Controller</p>"},{"location":"advanced_guides/#performance","title":"Performance","text":"<p>Highlight the performance numbers especially for scaling and across single versus multiple machines</p>"},{"location":"advanced_guides/#apache-beam-runners","title":"Apache Beam runners","text":"<p>Apache Beam supports a range of different runners depending on the deployment architecture. </p> <ul> <li>Describe the different beam runners</li> <li>How to configure via the pipeline</li> <li>Important config options e.g. setting memory etc</li> </ul>"},{"location":"advanced_guides/#deployment-patterns","title":"Deployment patterns**","text":"Approach Scenarios Considerations RDBMS using \"lossy\" schema (defined as ViewDefinition Resources) Using a relational database to power dashboards or reporting By design this will provide a constrained set of variables in the views Distributed \"lossless\" parquet based DWH and distributed query engine Need for a horizontally scalable architecture Will need to manage both distributed storage (Parquet) and a distributed query engine Non-distributed \"lossless\" parquet based DWH Want to leverage parquet with a non-distributed OLAP database engine (such as duckdb) xxx"},{"location":"advanced_guides/#web-control-panel","title":"Web Control Panel","text":"<p>The web control panel is a basic spring application provided to make interacting with the pipeline controller easier. It is not designed to be a full production ready \u201cweb admin\u201d panel.</p> <p>It has the following features:</p> <ul> <li>Initiate full and incremental pipeline runs</li> <li>Monitor errors when running pipelines</li> <li>Recreate view tables</li> <li>View configuration settings</li> <li>Access sample jupyter notebooks and ViewDefinition editor</li> </ul>"},{"location":"advanced_guides/#querying-exported-parquet-files","title":"Querying exported Parquet files","text":"<p>Assumes that you have completed ....</p>"},{"location":"advanced_guides/#handle-nested-fields","title":"Handle nested fields","text":"<p>One major challenge when querying exported data is that FHIR resources have many nested fields. One approach is to use <code>LATERAL VIEW</code> with <code>EXPLODE</code> to flatten repeated fields and then filter for specific values of interest.</p>"},{"location":"advanced_guides/#example-queries","title":"Example queries","text":"<p>The following queries explore the sample data loaded when using a local test server. They leverage <code>LATERAL VIEW</code> with <code>EXPLODE</code> to flatten the Observation.code.coding repeated field and filter for specific observation codes.</p> <p>Note that the synthetic sample data simulates HIV patients. Observations for HIV viral load use the following code, which is not the actual LOINC code:</p> <pre><code>[\n   {\n      \"id\":null,\n      \"coding\":[\n         {\n            \"id\":null,\n            \"system\":\"http://loinc.org\",\n            \"version\":null,\n            \"code\":\"856AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\",\n            \"display\":\"HIV viral load\",\n            \"userSelected\":null\n         }\n      ],\n      \"text\":\"HIV viral load\"\n   }\n]\n</code></pre>"},{"location":"advanced_guides/#flattening-a-resource-table-based-on-a-field","title":"Flattening a resource table based on a field","text":"<p>Let's say we are interested only in certain observation codes. For working with repeated fields like <code>Observation.code.coding</code> sometime it is easier to first \"flatten\" the table on that field. Conceptually this means that an Observation row with say 4 codes will be repeated 4 times where each row has exactly one of those 4 values. Here is a query that does that selecting only rows with \"viral load\" observations (code 856): </p> <p><pre><code>SELECT O.id AS obs_id, OCC.`system`, OCC.code, O.status AS status, O.value.quantity.value AS value\nFROM Observation AS O LATERAL VIEW explode(code.coding) AS OCC \nWHERE OCC.`system` = 'http://loinc.org'\n  AND OCC.code LIKE '856A%'\nLIMIT 4;\n</code></pre> Sample output: <pre><code>+---------+-------------------+---------------------------------------+---------+-----------+\n| obs_id  |      system       |                 code                  | status  |   value   |\n+---------+-------------------+---------------------------------------+---------+-----------+\n| 10393   | http://loinc.org  | 856AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA  | final   | 224650.0  |\n| 12446   | http://loinc.org  | 856AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA  | final   | 823010.0  |\n| 14456   | http://loinc.org  | 856AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA  | final   | 166100.0  |\n| 15991   | http://loinc.org  | 856AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA  | final   | 64630.0   |\n+---------+-------------------+---------------------------------------+---------+-----------+\n</code></pre></p>"},{"location":"advanced_guides/#find-patients-with-an-observed-viral-load-higher-than-a-threshold","title":"Find patients with an observed viral load higher than a threshold","text":"<p>Now, let's say we are interested only in cases with high viral load; and for each patient we need some demographic information too. We can use the flat table we created above and join it with the Patient resource table:</p> <pre><code>SELECT P.id AS pid, P.name.family AS family, P.gender AS gender, O.id AS obs_id, OCC.`system`,\n  OCC.code, O.status AS status, O.value.quantity.value AS value\nFROM Patient AS P, Observation AS O LATERAL VIEW explode(code.coding) AS OCC \nWHERE P.id = O.subject.PatientId\n  AND OCC.`system` = 'http://loinc.org'\n  AND OCC.code LIKE '856A%'\n  AND O.value.quantity.value &gt; 10000\nLIMIT 4;\n</code></pre> <p>Sample output:</p> <pre><code>+--------+-----------------------------------+---------+---------+-------------------+---------------------------------------+---------+-----------+\n|  pid   |              family               | gender  | obs_id  |      system       |                 code                  | status  |   value   |\n+--------+-----------------------------------+---------+---------+-------------------+---------------------------------------+---------+-----------+\n| 10091  | [\"Fritsch593\"]                    | male    | 10393   | http://loinc.org  | 856AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA  | final   | 224650.0  |\n| 11689  | [\"Dickinson688\"]                  | male    | 12446   | http://loinc.org  | 856AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA  | final   | 823010.0  |\n| 13230  | [\"Jerde200\",\"Ruecker817\"]         | female  | 14456   | http://loinc.org  | 856AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA  | final   | 166100.0  |\n| 15315  | [\"Pfeffer420\",\"Pfannerstill264\"]  | female  | 15991   | http://loinc.org  | 856AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA  | final   | 64630.0   |\n+--------+-----------------------------------+---------+---------+-------------------+---------------------------------------+---------+-----------+\n</code></pre>"},{"location":"advanced_guides/#count-all-viral-load-observations","title":"Count all viral-load observations","text":"<pre><code>SELECT COUNT(0)\nFROM (\n  SELECT P.id AS pid, P.name.family AS family, P.gender AS gender, O.id AS obs_id, OCC.`system`,\n    OCC.code, O.status AS status, O.value.quantity.value\n  FROM Patient AS P, Observation AS O LATERAL VIEW explode(code.coding) AS OCC \n  WHERE P.id = O.subject.PatientId\n    AND OCC.`system` = 'http://loinc.org'\n    AND OCC.code LIKE '856A%'\n);\n</code></pre> <p>Sample output:</p> <pre><code>+-----------+\n| count(0)  |\n+-----------+\n| 265       |\n+-----------+\n</code></pre>"},{"location":"advanced_guides/#use-views-to-reduce-complexity","title":"Use Views to reduce complexity","text":"<p>Once you have a query that filters to the data you're interested in, create a view with a simpler schema to work with in the future. This is a good way to create building blocks to combine with other data and create more complex queries.</p>"},{"location":"advanced_guides/#observations-of-patients-starting-an-anti-retroviral-plan-in-2010","title":"Observations of patients starting an Anti-retroviral plan in 2010","text":"<pre><code>SELECT\n  O.id AS obs_id, OCC.`system`, OCC.code, O.status AS status,\n  OVCC.code AS value_code, O.subject.PatientId AS patient_id\nFROM Observation AS O LATERAL VIEW explode(code.coding) AS OCC\n  LATERAL VIEW explode(O.value.codeableConcept.coding) AS OVCC\nWHERE OCC.code LIKE '1255%'\n  AND OVCC.code LIKE \"1256%\"\n  AND YEAR(O.effective.dateTime) = 2010\nLIMIT 1;\n</code></pre> <p>Sample output:</p> <pre><code>+---------+-------------------+---------------------------------------+---------+---------------------------------------+-------------+\n| obs_id  |      system       |                 code                  | status  |              value_code               | patient_id  |\n+---------+-------------------+---------------------------------------+---------+---------------------------------------+-------------+\n| 33553   | http://loinc.org  | 1255AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA  | final   | 1256AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA  | 32003       |\n+---------+-------------------+---------------------------------------+---------+---------------------------------------+-------------+\n</code></pre>"},{"location":"advanced_guides/#create-a-corresponding-view","title":"Create a corresponding view","text":"<pre><code>CREATE VIEW obs_arv_plan AS\nSELECT\n  O.id AS obs_id, OCC.`system`, OCC.code, O.status AS status,\n  OVCC.code AS value_code, O.subject.PatientId AS patient_id\nFROM Observation AS O LATERAL VIEW explode(code.coding) AS OCC\n  LATERAL VIEW explode(O.value.codeableConcept.coding) AS OVCC\nWHERE OCC.code LIKE '1255%'\n  AND OVCC.code LIKE \"1256%\"\n  AND YEAR(O.effective.dateTime) = 2010;\n</code></pre>"},{"location":"advanced_guides/#count-cases-of-anti-retroviral-plans-started-in-2010","title":"Count cases of Anti-retroviral plans started in 2010","text":"<pre><code>SELECT COUNT(0) FROM obs_arv_plan ;\n</code></pre> <p>Sample output:</p> <pre><code>+-----------+\n| count(0)  |\n+-----------+\n| 2         |\n+-----------+\n</code></pre>"},{"location":"advanced_guides/#compare-patient-data-with-view-based-on-observations","title":"Compare Patient data with view based on observations","text":"<pre><code>SELECT P.id AS pid, P.name.family AS family, P.gender AS gender, COUNT(0) AS num_start\nFROM Patient P, obs_arv_plan\nWHERE P.id = obs_arv_plan.patient_id\nGROUP BY P.id, P.name.family, P.gender\nORDER BY num_start DESC\nLIMIT 10;\n</code></pre> <p>Sample output:</p> <pre><code>+--------+-------------------+---------+------------+\n|  pid   |      family       | gender  | num_start  |\n+--------+-------------------+---------+------------+\n| 20375  | [\"VonRueden376\"]  | male    | 1          |\n| 32003  | [\"Terry864\"]      | male    | 1          |\n+--------+-------------------+---------+------------+\n</code></pre>"},{"location":"contributing/","title":"How to Contribute","text":"<p>We'd love to accept your patches and contributions to this project. There are just a few small guidelines you need to follow.</p>"},{"location":"contributing/#contributor-license-agreement","title":"Contributor License Agreement","text":"<p>Contributions to this project must be accompanied by a Contributor License Agreement (CLA). You (or your employer) retain the copyright to your contribution; this simply gives us permission to use and redistribute your contributions as part of the project. Head over to https://cla.developers.google.com/ to see your current agreements on file or to sign a new one.</p> <p>You generally only need to submit a CLA once, so if you've already submitted one (even if it was for a different project), you probably don't need to do it again.</p>"},{"location":"contributing/#code-reviews","title":"Code reviews","text":"<p>All submissions by non-project members, require review. We use GitHub pull requests for this purpose. Consult GitHub Help for more information on using pull requests. We use GitHub for issue tracking.</p>"},{"location":"contributing/#community-guidelines","title":"Community Guidelines","text":"<p>This project follows Google's Open Source Community Guidelines.</p>"},{"location":"getting_started/","title":"Getting Started","text":"<p>In this section we will run through concepts related to FHIR Data Pipes and instructions for getting started quickly.</p>"},{"location":"getting_started/#fhir-data-pipes-concepts","title":"FHIR Data Pipes concepts","text":"<p>FHIR Data Pipes provides pipelines for transforming data from a FHIR server into a Parquet format that you can use for data analysis and visualization. It can also be used to move data from one FHIR server to another. </p> <p>FHIR Data Pipes is built on technology designed for horizontal scalability and has multiple deployment options depending on your use case and requirements.</p>"},{"location":"getting_started/#install-fhir-data-pipes","title":"Install FHIR Data Pipes","text":"<p>There are a number of ways to FHIR Data Pipes can be installed and deployed on a number of different platforms and using different architectural patterns depending on the environment, amount of data and requirements for horizontal scalability.</p> <p>You can find additional information and instructions in the following sections:</p> <ul> <li>Configuring the FHIR Data Pipes Pipeline and Controller</li> <li>FHIR Data Pipes docker containers</li> <li>Single machine deployment (for quick set-up)</li> </ul> <p>Once you have set-up and deployed the FHIR Data Pipes Pipelines, you can explore different tutorials for working with FHIR Data Pipes.</p>"},{"location":"installation_controller/","title":"Installation controller","text":""},{"location":"installation_controller/#set-up-fhir-data-pipes","title":"Set-up FHIR Data Pipes","text":"<p>The pipelines directory contains code to transform data from a FHIR source to either Apache Parquet files for analysis or another FHIR store for data integration.</p> <p>See this page if you are using OpenMRS.</p> <p>Reading data from a FHIR Source</p> <p>The following options are supported for reading the FHIR source data (input data):</p> <ul> <li> <p>FHIR-Search: This mode uses FHIR Search APIs to select resources to     copy, retrieves them as FHIR resources, and transfers the data via FHIR APIs     or Parquet files. This mode should work with any FHIR server or Facade that has an FHIR Search API end-point</p> </li> <li> <p>ndjson: This mode uses ndjson bundles as inputs into the Pipeline. These could be generated using the Bulk Export API or other data transformation methods</p> </li> <li> <p>JDBC: This mode uses the Java Database Connectivity (JDBC)     API to read     FHIR resources directly from the database of a HAPI FHIR server configured to use a PostgreSQL     database     or an OpenMRS instance using MySQL.     Note: JDBC support beyond HAPI FHIR and OpenMRS is not currently     planned. Our long-term approach for a generic high-throughput alternative is to use     the FHIR Bulk Export API.</p> </li> </ul> <p>There are two options for transforming the data (output):</p> <ul> <li>\"Lossless\" schema as Parquet files: Outputs the FHIR resources as Parquet files, using the     SQL-on-FHIR schema.</li> <li>FHIR: Copies the FHIR resources to another FHIR server using FHIR APIs.</li> </ul>"},{"location":"installation_controller/#setup","title":"Setup","text":"<p>Clone the FHIR Data Pipes project to your machine. Set the <code>utils</code> directory to world-readable: <code>chmod -R 755 ./utils</code>. Build binaries by running <code>mvn clean install</code> from the root directory of the repository.</p>"},{"location":"installation_controller/#run-the-pipeline","title":"Run the pipeline","text":"<p>Run the pipeline directly using the <code>java</code> command:</p> run pipeline<pre><code>java -cp ./pipelines/batch/target/batch-bundled-0.1.0-SNAPSHOT.jar \\\n    org.openmrs.analytics.FhirEtl \\\n    --fhirServerUrl=http://example.org/fhir \\\n    --[see additional parameters below]\n</code></pre> <p>Add the necessary parameters depending on your use case. The methods used for reading the source FHIR server and outputting the data depend on the parameters used. You can output to both Parquet files and a FHIR server by including the required parameters for both.</p>"},{"location":"installation_controller/#parameters","title":"Parameters","text":"<p>This section documents the parameters used by the various pipelines. For more information on parameters, see [TO DO: Add page] <code>FhirEtlOptions</code>.</p>"},{"location":"installation_controller/#common-parameters","title":"Common parameters","text":"<p>These parameters are used regardless of other pipeline options.</p> <ul> <li><code>fhirServerUrl</code> - The base URL of the source FHIR server. Required.</li> <li><code>fhirServerUserName</code> - The HTTP Basic Auth username to access the FHIR         server APIs. Default: <code>admin</code></li> <li><code>fhirServerPassword</code> - The HTTP Basic Auth password to access the FHIR     server APIs. Default: <code>Admin123</code></li> <li><code>resourceList</code> - A comma-separated list of FHIR     resources to include in the     pipeline. Default: <code>Patient,Encounter,Observation</code></li> <li><code>runner</code> - The Apache Beam     Runner     to use. Pipelines supports DirectRunner and FlinkRunner by default; other     runners can be enabled by Maven profiles, e.g., DataflowRunner. Default: <code>DirectRunner</code></li> </ul>"},{"location":"installation_controller/#fhir-search-input-parameters","title":"FHIR-Search input parameters","text":"<p>The pipeline will use FHIR-Search to fetch data as long as <code>jdbcModeEnabled</code> is unset or false.</p> <ul> <li><code>batchSize</code> - The number of resources to fetch in each API call. Default:     <code>100</code></li> </ul>"},{"location":"installation_controller/#jdbc-input-parameters","title":"JDBC input parameters","text":"<p>JDBC mode is used if <code>jdbcModeEnabled=true</code>.</p> <p>To use JDBC mode, first create a copy of hapi-postgres-config.json and edit the values to match your database server.</p> <p>Next, include the following parameters:</p> <ul> <li><code>jdbcModeEnabled=true</code></li> <li><code>fhirDatabaseConfigPath=./path/to/config.json</code></li> </ul> <p>If you are using a HAPI FHIR server, also include:</p> <ul> <li><code>jdbcModeHapi=true</code></li> <li><code>jdbcDriverClass=org.postgresql.Driver</code></li> </ul> <p>All JDBC parameters:</p> <ul> <li><code>jdbcModeEnabled</code> - If true, uses JDBC mode. Default: <code>false</code></li> <li><code>fhirDatabaseConfigPath</code> - Path to the FHIR database config for JDBC mode.     Default: <code>../utils/hapi-postgres-config.json</code></li> <li><code>jdbcModeHapi</code> - If true (with <code>jdbcModeEnabled</code>), uses JDBC mode for a HAPI     source server. Default: <code>false</code></li> <li><code>jdbcFetchSize</code> - The fetch size of each JDBC database query. Default:     <code>10000</code></li> <li><code>jdbcMaxPoolSize</code> - The maximum number of database connections. Default:     <code>50</code></li> <li><code>jdbcDriverClass</code> - The JDBC driver to use. Should be set to     <code>org.postgresql.Driver</code> for HAPI FHIR Postgres access. Default:     <code>com.mysql.cj.jdbc.Driver</code></li> </ul>"},{"location":"installation_controller/#parquet-output-parameters","title":"Parquet output parameters","text":"<p>Parquet files are output when <code>outputParquetPath</code> is set.</p> <ul> <li><code>outputParquetPath</code> - The file path to write Parquet files to, e.g.,     <code>./tmp/parquet/</code>. Default: empty string, which does not output Parquet files.</li> <li><code>secondsToFlushParquetFiles</code> - The number of seconds to wait before flushing     all Parquet writers with non-empty content to files. Use <code>0</code> to disable.     Default: <code>3600</code>.</li> <li><code>rowGroupSizeForParquetFiles</code> - The approximate size in bytes of the     row-groups in Parquet files. When this size is reached, the content is     flushed to disk. This is not used if there are less than 100 records. Use     <code>0</code> to use the default Parquet row-group size. Default: <code>0</code>.</li> </ul>"},{"location":"installation_controller/#fhir-output-parameters","title":"FHIR output parameters","text":"<p>Resources will be copied to the FHIR server specified in <code>fhirSinkPath</code> if that field is set.</p> <ul> <li><code>fhirSinkPath</code> - A base URL to a target FHIR server, or the relative path of     a GCP FHIR store, e.g. <code>http://localhost:8091/fhir</code> for a FHIR server or     <code>projects/PROJECT/locations/LOCATION/datasets/DATASET/fhirStores/FHIR-STORE-NAME</code>     for a GCP FHIR store. If using a GCP FHIR store,     see here for setup information.     default: none, resources are not copied</li> <li><code>sinkUserName</code> - The HTTP Basic Auth username to access the FHIR sink. Not     used for GCP FHIR stores.</li> <li><code>sinkPassword</code> - The HTTP Basic Auth password to access the FHIR sink. Not     used for GCP FHIR stores.</li> </ul>"},{"location":"installation_controller/#a-note-about-beam-runners","title":"A note about Beam runners","text":"<p>If the pipeline is run on a single machine (i.e., not on a distributed cluster), for large datasets consider using a production grade runner like Flink. </p> <p>This can be done by adding the parameter <code>--runner=FlinkRunner</code> (use <code>--maxParallelism</code> and <code>--parallelism</code> to control parallelism). This should not give a significant run time improvement but may avoid some of the memory issues of <code>DirectRunner</code>.</p> <p>[To-do: Read more in the \u201cApache Beam Runners\u201d guide]</p>"},{"location":"installation_controller/#example-configurations","title":"Example configurations","text":"<p>These examples are set up to work with local test servers.</p>"},{"location":"installation_controller/#fhir-search-to-parquet-files","title":"FHIR Search to Parquet files","text":"<p>Example run:</p> <pre><code>java -cp ./pipelines/batch/target/batch-bundled-0.1.0-SNAPSHOT.jar \\\n    org.openmrs.analytics.FhirEtl \\\n    --fhirServerUrl=http://localhost:8091/fhir \\\n    --outputParquetPath=/tmp/TEST/ \\\n    --resourceList=Patient,Encounter,Observation\n</code></pre>"},{"location":"installation_controller/#hapi-fhir-jdbc-to-a-fhir-server","title":"HAPI FHIR JDBC to a FHIR server","text":"<p>Example run:</p> <pre><code>java -cp ./pipelines/batch/target/batch-bundled-0.1.0-SNAPSHOT.jar \\\n    org.openmrs.analytics.FhirEtl \\\n    --fhirServerUrl=http://localhost:8091/fhir \\\n    --resourceList=Patient,Encounter,Observation \\\n    --fhirDatabaseConfigPath=./utils/hapi-postgres-config.json \\\n    --jdbcModeEnabled=true --jdbcModeHapi=true \\\n    --jdbcMaxPoolSize=50 --jdbcFetchSize=1000 \\\n    --jdbcDriverClass=org.postgresql.Driver \\\n    --fhirSinkPath=http://localhost:8099/fhir \\\n    --sinkUserName=hapi --sinkPassword=hapi\n</code></pre>"},{"location":"installation_docker/","title":"FHIR Data Pipes in a Docker Container","text":"<p>The easiest way to get started is to set up a [single machine deployment] using a docker image provided in the repository.</p> <p>By default we use the Basic \"Single Machine\"  Docker Compose Configuration. For descriptions of the different available compose configurations see here</p> <p>By default, the Single Machine docker compose configuration will:</p> <ul> <li>Bring up the FHIR Pipelines Controller plus a Spark Thrift Server</li> <li>Start the Web Control Panel for interacting with the Controller for running full and incremental Pipelines and registering new views</li> <li>Generate Parquet files for in the /docker/dwh directory </li> <li>Run the SQL-on-FHIR queries defined in /config/views/*.sql and register these via the Thrift Server</li> <li>Generate flattened views (defined in /config/views/*.json) in the database configured by sinkDbConfigPath (you will need to ensure the database is created)</li> </ul> <p>The rest of this guide covers setting up and deploying an end-to-end environment for running analytics queries on FHIR Data. It includes optional instructions for provisioning a FHIR Server with sample data.</p>"},{"location":"installation_docker/#requirements","title":"Requirements","text":"<ul> <li>Docker - If you are using Linux, Docker must be in sudoless mode</li> <li>Docker Compose</li> <li>The FHIR Data Pipes repository, cloned onto the host machine</li> </ul>"},{"location":"installation_pipeline/","title":"FHIR Data Pipes Pipelines","text":"<p>The batch directory contains code for a Java JAR which transforms data from a FHIR server to either Apache Parquet files for analysis or another FHIR store for data integration.</p> <p>There are two options for reading the source FHIR server (input):</p> <ul> <li>FHIR-Search: This mode uses FHIR Search APIs to select resources to copy,   retrieves them as FHIR resources, and transfers the data via FHIR APIs or   Parquet files. This mode should work with most FHIR servers and has been   tested with HAPI FHIR server and GCP FHIR store.</li> <li>JDBC: This mode uses the   Java Database Connectivity (JDBC) API   to read FHIR resources directly from the database of a FHIR server. It's   tested with   HAPI FHIR server using PostgreSQL database   or an OpenMRS instance using MySQL. Note: JDBC support   beyond HAPI FHIR and OpenMRS is not currently planned. Our long-term approach   for a generic high-throughput alternative is to use the   FHIR Bulk Export API.</li> </ul> <p>There are two options for transforming the data (output):</p> <ul> <li>Parquet: Outputs the FHIR resources as Parquet files, using the   SQL-on-FHIR schema.</li> <li>FHIR: Copies the FHIR resources to another FHIR server using FHIR APIs.</li> </ul>"},{"location":"installation_pipeline/#setup","title":"Setup","text":"<ol> <li>Clone     the FHIR Data Pipes project to     your machine.</li> <li>Set the <code>utils</code> directory to world-readable: <code>chmod -R 755 ./utils</code>.</li> <li>Build binaries by running <code>mvn clean install</code> from the root directory of the     repository.</li> </ol>"},{"location":"installation_pipeline/#run-the-pipeline","title":"Run the pipeline","text":"<p>Run the pipeline directly using the <code>java</code> command:</p> <pre><code>java -jar ./pipelines/batch/target/batch-bundled.jar \\\n    com.google.fhir.analytics.FhirEtl \\\n    --fhirServerUrl=http://example.org/fhir \\\n    --outputParquetPath=/tmp/parquet/\n    --[see additional parameters below]\n</code></pre> <p>Add the necessary parameters depending on your use case. The methods used for reading the source FHIR server and outputting the data depend on the parameters used. You can output to both Parquet files and a FHIR server by including the required parameters for both.</p>"},{"location":"installation_pipeline/#parameters","title":"Parameters","text":"<p>This section documents the parameters used by the various pipelines. For more information on parameters, see <code>FhirEtlOptions</code> or run the pipeline with the <code>help</code> option: <code>java -jar ./batch/target/batch-bundled.jar --help=FhirEtlOptions</code>.</p>"},{"location":"installation_pipeline/#common-parameters","title":"Common parameters","text":"<p>These parameters are used regardless of other pipeline options.</p> <ul> <li><code>resourceList</code> - A comma-separated list of   FHIR resources to include in the   pipeline. Default: <code>Patient,Encounter,Observation</code></li> <li><code>runner</code> -   The Apache Beam Runner   to use. Pipelines supports <code>DirectRunner</code> and <code>FlinkRunner</code> by default; other   runners can be enabled by Maven profiles, e.g.,   DataflowRunner.   See also   A note about Beam runners.   Default: <code>DirectRunner</code></li> </ul>"},{"location":"installation_pipeline/#fhir-search-input-parameters","title":"FHIR-Search input parameters","text":"<p>The pipeline will use FHIR-Search to fetch data as long as <code>jdbcModeEnabled</code> and <code>jdbcModeHapi</code> are unset or false.</p> <ul> <li><code>fhirServerUrl</code> - The base URL of the source FHIR server. Required.</li> <li><code>fhirServerUserName</code> - The HTTP Basic Auth username to access the FHIR server   APIs. Default: <code>admin</code></li> <li><code>fhirServerPassword</code> - The HTTP Basic Auth password to access the FHIR server   APIs. Default: <code>Admin123</code></li> <li><code>batchSize</code> - The number of resources to fetch in each API call. Default:   <code>100</code></li> </ul>"},{"location":"installation_pipeline/#jdbc-input-parameters","title":"JDBC input parameters","text":"<p>JDBC mode is used if a JDBC flag is <code>true</code>.</p> <p>To use JDBC mode:</p> <p>1: Create a copy of hapi-postgres-config.json and edit the values to match your database server.</p> <p>2: Enable JDBC mode for your source server:</p> <ul> <li>OpenMRS</li> <li><code>jdbcModeEnabled=true</code></li> <li>HAPI FHIR server</li> <li><code>jdbcModeHapi=true</code></li> </ul> <p>3: Specify the path to your config file.</p> <ul> <li><code>fhirDatabaseConfigPath=./path/to/config.json</code></li> </ul> <p>All JDBC parameters:</p> <ul> <li><code>jdbcModeHapi</code> - If true, uses JDBC mode for HAPI FHIR server. Default:   <code>false</code></li> <li><code>jdbcModeEnabled</code> - If true, uses JDBC mode for OpenMRS. Default: <code>false</code></li> <li><code>fhirDatabaseConfigPath</code> - Path to the FHIR database config for JDBC mode.   Default: <code>../utils/hapi-postgres-config.json</code></li> <li><code>jdbcFetchSize</code> - The fetch size of each JDBC database query. Default: <code>10000</code></li> <li><code>jdbcMaxPoolSize</code> - The maximum number of database connections. Default: <code>50</code></li> </ul>"},{"location":"installation_pipeline/#parquet-output-parameters","title":"Parquet output parameters","text":"<p>Parquet files are output when <code>outputParquetPath</code> is set.</p> <ul> <li><code>outputParquetPath</code> - The file path to write Parquet files to, e.g.,   <code>./tmp/parquet/</code>. Default: empty string, which does not output Parquet files.</li> <li><code>secondsToFlushParquetFiles</code> - The number of seconds to wait before flushing   all Parquet writers with non-empty content to files. Use <code>0</code> to disable.   Default: <code>3600</code>.</li> <li><code>rowGroupSizeForParquetFiles</code> - The approximate size in bytes of the   row-groups in Parquet files. When this size is reached, the content is flushed   to disk. This is not used if there are less than 100 records. Use <code>0</code> to use   the default Parquet row-group size. Default: <code>0</code>.</li> </ul>"},{"location":"installation_pipeline/#fhir-output-parameters","title":"FHIR output parameters","text":"<p>Resources will be copied to the FHIR server specified in <code>fhirSinkPath</code> if that field is set.</p> <ul> <li><code>fhirSinkPath</code> - A base URL to a target FHIR server, or the relative path of a   GCP FHIR store, e.g. <code>http://localhost:8091/fhir</code> for a FHIR server or   <code>projects/PROJECT/locations/LOCATION/datasets/DATASET/fhirStores/FHIR-STORE-NAME</code>   for a GCP FHIR store. If using a GCP FHIR store,   see here for setup information.   default: none, resources are not copied</li> <li><code>sinkUserName</code> - The HTTP Basic Auth username to access the FHIR sink. Not   used for GCP FHIR stores.</li> <li><code>sinkPassword</code> - The HTTP Basic Auth password to access the FHIR sink. Not   used for GCP FHIR stores.</li> </ul>"},{"location":"installation_pipeline/#a-note-about-beam-runners","title":"A note about Beam runners","text":"<p>If the pipeline is run on a single machine (i.e., not on a distributed cluster), for large datasets consider using a production grade runner like Flink. This can be done by adding the parameter <code>--runner=FlinkRunner</code> (use <code>--maxParallelism</code> and <code>--parallelism</code> to control parallelism). This may avoid some of the memory issues of <code>DirectRunner</code>.</p>"},{"location":"installation_pipeline/#example-configurations","title":"Example configurations","text":"<p>These examples are set up to work with local test servers.</p>"},{"location":"installation_pipeline/#fhir-search-to-parquet-files","title":"FHIR Search to Parquet files","text":"<p>Example run:</p> <pre><code>java -cp ./pipelines/batch/target/batch-bundled.jar \\\n    com.google.fhir.analytics.FhirEtl \\\n    --fhirServerUrl=http://localhost:8091/fhir \\\n    --outputParquetPath=/tmp/TEST/ \\\n    --resourceList=Patient,Encounter,Observation\n</code></pre>"},{"location":"installation_pipeline/#hapi-fhir-jdbc-to-a-fhir-server","title":"HAPI FHIR JDBC to a FHIR server","text":"<p>Example run:</p> <pre><code>java -cp ./pipelines/batch/target/batch-bundled.jar \\\n    com.google.fhir.analytics.FhirEtl \\\n    --fhirServerUrl=http://localhost:8091/fhir \\\n    --resourceList=Patient,Encounter,Observation \\\n    --fhirDatabaseConfigPath=./utils/hapi-postgres-config.json \\\n    --jdbcModeEnabled=true --jdbcModeHapi=true \\\n    --jdbcMaxPoolSize=50 --jdbcFetchSize=1000 \\\n    --jdbcDriverClass=org.postgresql.Driver \\\n    --fhirSinkPath=http://localhost:8099/fhir \\\n    --sinkUserName=hapi --sinkPassword=hapi\n</code></pre>"},{"location":"installation_pipeline/#how-to-query-the-data-warehouse","title":"How to query the data warehouse","text":"<p>To query Parquet files, load them into a compatible data engine such as Apache Spark. The single machine Docker Compose configuration runs the pipeline and loads data into an Apache Spark Thrift server for you.</p>"},{"location":"release_process/","title":"Semantic versioning","text":"<p>Versioning across all Open Health Stack components is based on the major.minor.patch scheme and respects Semantic Versioning.</p> <p>Respecting Semantic Versioning is important for multiple reasons:</p> <ul> <li>It guarantees simple minor version upgrades, as long as you only use the public APIs</li> <li>A new major version is an opportunity to thoroughly document breaking changes</li> <li>A new major/minor version is an opportunity to communicate new features through a blog post</li> </ul>"},{"location":"release_process/#major-versions","title":"Major versions","text":"<p>The major version number is incremented on every breaking change.</p> <p>Whenever a new major version is released, we publish:</p> <ul> <li>a blog post with feature highlights, major bug fixes, breaking changes, and upgrade instructions.</li> <li>an exhaustive changelog entry via the release notes</li> </ul>"},{"location":"release_process/#minor-versions","title":"Minor versions","text":"<p>The minor version number is incremented on every significant retro-compatible change.</p> <p>Whenever a new minor version is released, we publish:</p> <ul> <li>an exhaustive changelog entry via the release notes</li> </ul>"},{"location":"release_process/#patch-versions","title":"Patch versions","text":"<p>The patch version number is incremented on bugfixes releases.</p> <p>Whenever a new patch version is released, we publish:</p> <ul> <li>an exhaustive changelog entry</li> </ul>"},{"location":"support/","title":"Support","text":"<p>On this page we've listed some ways you can get technical support and Open Health Stack communities and forums that you can be a part of.</p> <p>Before participating please read our code of conduct that we expect all community members to adhere too.</p>"},{"location":"support/#developer-calls","title":"Developer calls","text":"<p>There are weekly Open Health Stack developer calls that you are welcome to join. These alternate between Android FHIR SDK and OHS server side (FHIR Data Pipes and Info Gateway). </p> <p>The schedule is below:</p> <ul> <li>Android FHIR SDK: Thursdays</li> <li>OHS Server Side: Thursday</li> </ul> <p>To sign-up, please email: hello-ohs[at]google.com</p>"},{"location":"support/#discussion-forums","title":"Discussion forums","text":"<p>We are in the process of setting up a dedicated discussion forum for Open Health Stack. In the meantime, you can reach out to"},{"location":"support/#stack-overflow","title":"Stack Overflow","text":"<p>Stack Overflow is a popular forum to ask code-level questions or if you're stuck with a specific error. Read through the existing questions tagged with open-health-stack or fhir-data-pipes or ask your own!</p>"},{"location":"support/#bugs-or-feature-reqeusts","title":"Bugs or Feature reqeusts","text":"<p>Before submitting a bug or filing a feature reqeust, please review the open issues on our github repository.</p> <p>If your issue is there, please add a comment. Otherwise, create a new issue to file a bug or submit a new feature request.</p> <p>Please review the \"contributing\" section.</p>"},{"location":"tutorial_add_dashboard/","title":"Visualize Parquet DWH with Apache Superset","text":"<p>Generating dashboards is a common task for many digital health projects. </p> <p>This tutorial shows you how to use the popular open source Apache Superset package to build a dashboard on top of the \"lossless\" parquet DWH created via the FHIR Data Pipes Single Machine Deployment Tutorial.</p> <p>The principles here will apply to any visualization or BI tool that has a hive connector.</p>"},{"location":"tutorial_add_dashboard/#prerequisites","title":"Prerequisites","text":"<ul> <li>A FHIR Data Pipes single machine deployment using [local test     servers] as the data source</li> <li>Run the full pipeline at least once before following this guide.</li> </ul>"},{"location":"tutorial_add_dashboard/#install-apache-superset","title":"Install Apache Superset","text":"<p>There are several ways to install Superset. A simple option is to install Superset locally using docker-compose. Once it is set-up, log in by visiting http://localhost:8088/ and using the default credentials username: <code>admin</code> password: <code>admin</code>.</p>"},{"location":"tutorial_add_dashboard/#connect-to-the-apache-spark-sql-data-source","title":"Connect to the Apache Spark SQL data source","text":"<ol> <li>In the upper-right corner, select Settings &gt; Database Connections.</li> <li>In the upper-right corner, select + Database.</li> </ol>"},{"location":"tutorial_add_dashboard/#create-the-connection-for-spark-sql","title":"Create the connection for Spark SQL","text":"<ol> <li>Under Supported Databases, select Apache Spark SQL.</li> <li>Under Display Name, enter \"Single Machine Test Data\" or another name     your choice.</li> <li> <p>Under SQLAlchemy URI, enter the following:</p> <pre><code>hive://hive@&lt;IP_address_of_docker_network_gateway&gt;:10001\n</code></pre> <p>To find the IP address, run:</p> <pre><code>docker network inspect bridge --format='{{json .IPAM.Config}}'\n</code></pre> </li> <li> <p>Select Test Connection. You should see a pop-up in the bottom-right     corner that says \"Connection looks good!\"</p> <ul> <li>If you get an error, double-check the IP address is correct. You may     also need to install the Apache Spark SQL database drivers.</li> </ul> </li> <li>Select Connect. Although you see an error: \"An error occurred while     creating databases: Fatal error\" the connection is still created.</li> <li>Close the Connect a database window by clicking outside of it and     refresh the Databases page. The database you just created should appear.</li> </ol>"},{"location":"tutorial_add_dashboard/#create-a-dashboard","title":"Create a dashboard","text":"<p>Create an empty dashboard so you can add charts to it as they are created.</p> <ol> <li>At the top of the Superset page select the Dashboard tab, then click +     Dashboard.</li> <li>Select the title area which reads [ untitled dashboard ] and give the     dashboard a name. The examples below use \"Sample Dashboard\".</li> <li>In the upper-right corner, select Save.</li> </ol>"},{"location":"tutorial_add_dashboard/#query-the-database-and-generate-charts","title":"Query the database and generate Charts","text":"<ol> <li>From the top tabs, select SQL &gt; SQL Lab.</li> <li>If this is your first time using SQL Lab, you should be in a blank, empty     tab. If not, click New tab or press <code>Control+T</code> to create one.</li> <li>On the left side under Database, select the database you created     earlier, for example Single Machine Test Data.</li> </ol>"},{"location":"tutorial_add_dashboard/#add-a-big-number","title":"Add a Big Number","text":"<p>This first chart shows the total number of patients as a \"Big Number\".</p> <ol> <li> <p>Replace the placeholder query <code>SELECT ...</code> with the following:</p> <pre><code>SELECT COUNT(*) FROM Patient;\n</code></pre> </li> <li> <p>Click Run. After a moment, the results of the query should appear.</p> </li> <li>In the Results section, click Create Chart. This brings you to a new     chart page.</li> <li>Under the Chart Source pane, click Create a dataset and name it     \"Patient Count Dataset\".</li> <li>Under the Data tab, switch to the 4k (Big Number) chart type.</li> <li>In the Query section, click the Metric section and set the following     values:<ol> <li>Column: <code>count(1)</code></li> <li>Aggregate: <code>MAX</code></li> </ol> </li> <li>Click Update Chart at the bottom of the pane.</li> </ol> <p>The chart updates to show a single large number of the count of patients.</p> <ol> <li>Click Save to bring up the Save chart dialog. Name the chart     \"Patient Count\" and under Add to Dashboard select the dashboard you     created previously.</li> </ol>"},{"location":"tutorial_add_dashboard/#add-a-pie-chart","title":"Add a pie chart","text":"<p>The next chart shows the split of patients by gender as a pie chart.</p> <ol> <li>Navigate back to SQL Lab. You should see the previous patient count query.<ul> <li>If you'd like to keep the patient count query, make a new tab and set     the database to \"Single Machine Test Data\" or the name you chose.</li> </ul> </li> <li> <p>Enter the following as the query:</p> <pre><code>SELECT `gender` AS `gender`,\n       count(`gender`) AS `COUNT(gender)`\nFROM Patient AS P\nGROUP BY `gender`\nORDER BY `COUNT(gender)` DESC;\n</code></pre> </li> <li> <p>Click Run. After a moment, the results of the query should appear.</p> </li> <li>In the Results section, click Create Chart. This brings you to a new     chart page.</li> <li>Under the Chart Source pane, click Create a dataset and name it     \"Gender Split Dataset\".</li> <li>Under the Data tab, switch to the Pie Chart chart type.</li> <li>In the Query section, set the following values:<ul> <li>Dimensions:<ol> <li>Column: <code>gender</code></li> </ol> </li> <li>Metric:<ol> <li>Column: <code>COUNT(gender)</code></li> <li>Aggregate: <code>MAX</code></li> </ol> </li> </ul> </li> <li>Click Update Chart at the bottom of the pane. The chart updates to show     a pie chart of patients by gender.</li> <li>Click Save to bring up the Save chart dialog. Name the chart \"Gender     Split\" and under Add to Dashboard select the dashboard you created     previously.</li> </ol>"},{"location":"tutorial_add_dashboard/#add-a-complex-bar-chart","title":"Add a complex bar chart","text":"<p>The next chart shows number of patients on HIV treatment by year and display that as a bar chart over time. The query is provided below.</p> <ul> <li>The codes 1255% and 1266% refer to HIV_Tx codes in the dataset.</li> </ul> <p>Learn more about querying data in SQL-on-FHIR.</p> <ol> <li>Navigate back to SQL Lab. You should see the previous patient count query.<ul> <li>If you'd like to keep the patient count query, make a new tab and set     the database to \"Single Machine Test Data\" or the name you chose.</li> </ul> </li> <li> <p>Enter the following as the query:</p> <pre><code>SELECT COUNT(*), YEAR(O.effective.dateTime)\nFROM Observation AS O LATERAL VIEW EXPLODE(code.coding) AS OCC LATERAL VIEW EXPLODE(O.value.codeableConcept.coding) AS OVCC\nWHERE OCC.code LIKE '1255%'\n  AND OVCC.code LIKE \"1256%\"\n  AND YEAR(O.effective.dateTime) &lt; 2023\nGROUP BY YEAR(O.effective.dateTime)\nORDER BY YEAR(O.effective.dateTime) ASC\n</code></pre> </li> </ol> <p>NOTE: Using the modifiable pre-defined flat views ), the same query can be simplified to the following:</p> <pre><code>SELECT COUNT(*), YEAR(o.obs_date)\nFROM observation_flat as o\nWHERE o.code LIKE '1255%'\n  AND o.val_code LIKE \"1256%\"\n  AND YEAR(o.obs_date) &lt; 2023\nGROUP BY YEAR(o.obs_date)\nORDER BY YEAR(o.obs_date) ASC \n</code></pre> <ol> <li>Click Run. After a moment, the results of the query should appear.</li> <li>In the Results section, click Create Chart. This brings you to a new     chart page.</li> <li>Under the Chart Source pane, click Create a dataset and name it \"HIV     Treatment Dataset\".</li> <li>Under the Data tab, switch to the Bar Chart chart type.</li> <li>In the Query section, set the following values:<ol> <li>X-Axis:<ol> <li>Custom SQL: <code>YEAR(date\\_time)</code></li> </ol> </li> <li>Metric:<ol> <li>Column: <code>patient\\_id</code></li> <li>Aggregate: <code>COUNT</code></li> </ol> </li> </ol> </li> <li>Click Update Chart at the bottom of the pane. The chart updates to show     a bar chart of patients undergoing HIV treatment per year.</li> <li>Click Save to bring up the Save chart dialog. Name the chart \"HIV     Treatment\" and under Add to Dashboard select the dashboard you created     previously.</li> </ol>"},{"location":"tutorial_add_dashboard/#edit-the-dashboard","title":"Edit the Dashboard","text":"<p>Navigate to the Dashboards section and select the Sample Dashboard you created. You should see something like this with the charts you created.</p> <p></p> <p>Create a dashboard with 2 rows and a header. First click Edit Dashboard then:</p> <ol> <li>In the right pane select the Layout Elements tab.</li> <li>Add a second row by dragging a Row element onto the canvas below the     existing row.</li> <li>Arrange the charts to have the Patient Count and Gender Split in the top     row, and HIV Treatment in the bottom row.</li> <li>Resize the top charts to use half of the width by dragging the edge of the     chart.</li> <li>Resize the HIV Treatment chart to take up the whole width of the row.</li> <li>Add a header element at the top and enter \"Key Program Metrics\".</li> </ol> <p>Your dashboard now looks something like this:</p> <p></p>"},{"location":"tutorial_add_dashboard/#update-the-dashboard","title":"Update the dashboard","text":"<p>To check that the dashboard is working properly, add a new patient to the FHIR Store and run the incremental pipeline.</p> <ol> <li> <p>Add a new patient to the server:</p> <pre><code>curl -X POST -H \"Content-Type: application/fhir+json; charset=utf-8\" \\\n     'http://localhost:8091/fhir/Patient/' -d '{\"resourceType\": \"Patient\"}'\n</code></pre> </li> <li> <p>Alternatively, use the HAPI FHIR server tester; go to the Patient resource     tester, click the CRUD Operations tab, paste the following resource     into the Contents field of the Create section, then click     Create.</p> <pre><code>{\"resourceType\": \"Patient\"}\n</code></pre> </li> <li> <p>Open the Pipeline Controller at http://localhost:8090 and run the     incremental pipeline.</p> </li> <li>Once the pipeline has finished, go to Superset and open your dashboard.     Click the ... button next to Edit Dashboard and then click Refresh     dashboard. You should see the patient count increase by 1.</li> </ol>"},{"location":"tutorial_add_dashboard/#learn-more","title":"Learn more","text":"<p>For more information about Superset, go to the Superset documentation.</p>"},{"location":"tutorial_lossy_example/","title":"Relational DWH using custom \"lossy\" schema","text":""},{"location":"tutorial_lossy_example/#overview","title":"Overview","text":"<p>In this tutorial you will learn how to configure and deploy FHIR Data Pipes to transform FHIR data into a Postgre Relational SQL data-warehouse using FHIR ViewDefinition Resources to define the custom \"lossy\" schema.</p>"},{"location":"tutorial_lossy_example/#requirements","title":"Requirements","text":"<ul> <li>A source HAPI FHIR server configured to use Postgres as its database<ul> <li>If you don't have a server, use a local test server by following the instructions to bring up a source HAPI FHIR server with Postgres</li> </ul> </li> <li>Docker<ul> <li>If you are using Linux, Docker must be in sudoless mode</li> </ul> </li> <li>Docker Compose - this guide assumes you are using the latest version </li> <li>The FHIR Data Pipes repository, cloned onto the host machine</li> </ul>"},{"location":"tutorial_lossy_example/#configure-the-fhir-pipelines-controller","title":"Configure the FHIR Pipelines Controller","text":"<p>Note: All file paths are relative to the root of the FHIR Data Pipes repository.</p> <p>NOTE: You need to configure only one of the following options:</p> <ol> <li>For FHIR Search API (works for any FHIR server): </li> <li>Open <code>docker/config/application.yaml</code> and edit the value of <code>fhirServerUrl</code> to match the FHIR server you are connecting to. </li> <li> <p>Comment out the <code>dbConfig</code> in this case.</p> </li> <li> <p>For direct DB access (specific to HAPI FHIR servers):</p> </li> <li>Comment out <code>fhirServerUrl</code></li> <li>Set <code>dbConfig</code> to the DB connection config file, e.g., <code>docker/config/hapi-postgres-config_local.json</code>; </li> <li>Edit the values in this file to match the database for the FHIR server you are connecting to.</li> </ol>"},{"location":"tutorial_lossy_example/#set-the-sinkdbconfigpath","title":"Set the sinkDbConfigPath","text":"<p>The sinkDb refers to the target database that will become the data warehouse.</p> <p>With the default config, you will create both Parquet files (under <code>dwhRootPrefix</code>) and flattened views in the database configured by <code>sinkDbConfigPath</code> here. </p> <p>Make sure to create the database referenced in the connection config file (default is a postgreSQL db named 'views'). You can do this with the following SQL query:</p> <p><pre><code>CREATE DATABASE views;\n</code></pre> which you can run in Postgres like this: <pre><code>PGPASSWORD=admin psql -h 127.0.0.1 -p 5432 -U admin postgres -c \"CREATE DATABASE views\"\n</code></pre></p> <p>For documentation of all config parameters, see here.</p> <p>If you are using the local test servers, things should work with the default values. If not, use the IP address of the Docker default bridge network. To find it, run the following command and use the \"Gateway\" value:</p> <pre><code>docker network inspect bridge | grep Gateway\n</code></pre> <p>The Single Machine docker configuration uses two environment variables, <code>DWH_ROOT</code> and <code>PIPELINE_CONFIG</code>, whose default values are defined in the .env file. To override them, set the variable before running the <code>docker-compose</code> command. For example, to override the <code>DWH_ROOT</code> environment variable, run the following:</p> <pre><code>DWH_ROOT=\"$(pwd)/&lt;path_to_dwh_directory&gt;\" docker compose -f docker/compose-controller-spark-sql-single.yaml up --force-recreate \n</code></pre>"},{"location":"tutorial_lossy_example/#run-the-single-machine-configuration","title":"Run the Single Machine configuration","text":"<p>To bring up the <code>docker/compose-controller-spark-sql-single.yaml</code> configuration for the first time or if you have run this container in the past and want to include new changes pulled into the repo, run:</p> <pre><code>docker compose -f docker/compose-controller-spark-sql-single.yaml up --force-recreate --build\n</code></pre> <p>Alternatively, to run without rebuilding use:</p> <pre><code>docker compose -f docker/compose-controller-spark-sql-single.yaml up --force-recreate\n</code></pre> <p>Alternatively, <code>docker/compose-controller-spark-sql.yaml</code> serves as a very simple example on how to integrate the Parquet output of Pipelines in a Spark cluster environment.</p> <p>Once started, the Pipelines Controller is available at <code>http://localhost:8090</code> and the Spark Thrift server is at <code>http://localhost:10001</code>.</p> <p>The first time you run the Pipelines Controller, you must manually start a Full Pipeline run. In a browser go to <code>http://localhost:8090</code> and click the Run Full button. </p> <p>After running the Full Pipeline, use the Incremental Pipeline to update the Parquet files and tables. By default it is scheduled to run every hour, or you can manually trigger it.</p> <p>If the Incremental Pipeline does not work, or you see errors like:</p> <pre><code>ERROR o.openmrs.analytics.PipelineManager o.openmrs.analytics.PipelineManager$PipelineThread.run:343 - exception while running pipeline: \npipeline-controller    | java.sql.SQLException: org.apache.hive.service.cli.HiveSQLException: Error running query: org.apache.spark.sql.AnalysisException: Unable to infer schema for Parquet. It must be specified manually.\n</code></pre> <p>try running <code>sudo chmod -R 755</code> on the Parquet file directory, by default located at <code>docker/dwh</code>.</p>"},{"location":"tutorial_lossy_example/#explore-the-resulting-schema-in-postgresql","title":"Explore the resulting schema in PostgreSQL","text":"<p>Connect to the PostgreSQL RDBMS via docker using the cmd: <code>docker exec -it &lt;container_name_or_id&gt; bash</code></p> <p>If using the default container (hapi-fhir-db) run: <code>docker exec -it hapi_fhir_db bash</code></p> <p>Using psql connect to the 'views'  database: <code>psql -U admin -d views</code></p> <p>To list the tables: <code>\\d</code></p> Schema Name Type Owner public condition_flat table admin public diagnostic_report_flat table admin public immunization_flat table admin public location_flat table admin public medication_request_flat table admin public observation_flat table admin public organization_flat table admin public practitioner_flat table admin public practitioner_role_flat table admin public procedure_flat table admin"},{"location":"tutorial_lossy_example/#querying-the-database","title":"Querying the database","text":"<p>Let's do some basic quality checks to make sure the data is uploaded properly (note table names are case insensitive).</p> <p>NOTES:</p> <ul> <li>This assumes that the data loaded is from this synthetic data set</li> <li>You will see that the number of patients and observations is higher than the count in the FHIR Server. This is due to the flattening</li> </ul> <p><pre><code>SELECT COUNT(0) FROM patient_flat;\n</code></pre> We should have exactly 114 patients: <pre><code>+-----------+\n| count     |\n+-----------+\n| 114       |\n+-----------+\n</code></pre></p> <p>Doing the same for observations: <pre><code>SELECT COUNT(0) FROM observation_flat;\n</code></pre> <pre><code>+-----------+\n| count  |\n+-----------+\n| 18343     |\n+-----------+\n</code></pre></p>"},{"location":"tutorial_lossy_example/#whats-next","title":"What's next","text":"<p>Now that you have</p>"},{"location":"tutorial_single_machine/","title":"Single machine deployment tutorial","text":"<p>The repository includes a \"Single Machine\" Docker Compose configuration which brings up the FHIR Pipelines Controller plus a Spark Thrift server, letting you more easily run Spark SQL queries on the Parquet files output by the Pipelines Controller.</p> <p>To learn how the Pipelines Controller works on its own, Try out the FHIR Pipelines Controller.</p>"},{"location":"tutorial_single_machine/#requirements","title":"Requirements","text":"<ul> <li>A source HAPI FHIR server configured to use Postgres as its database<ul> <li>If you don't have a server, use a local test server by following the instructions to bring up a source HAPI FHIR server with Postgres</li> </ul> </li> <li>Docker<ul> <li>If you are using Linux, Docker must be in sudoless mode</li> </ul> </li> <li>Docker Compose - this guide assumes you are using the latest version </li> <li>The FHIR Data Pipes repository, cloned onto the host machine</li> </ul>"},{"location":"tutorial_single_machine/#configure-the-fhir-pipelines-controller","title":"Configure the FHIR Pipelines Controller","text":"<p>Note: All file paths are relative to the root of the FHIR Data Pipes repository.</p> <p>NOTE: You need to configure only one of the following options:</p> <ol> <li>For FHIR Search API (works for any FHIR server): </li> <li>Open <code>docker/config/application.yaml</code> and edit the value of <code>fhirServerUrl</code> to match the FHIR server you are connecting to. </li> <li> <p>Comment out the <code>dbConfig</code> in this case.</p> </li> <li> <p>For direct DB access (specific to HAPI FHIR servers):</p> </li> <li>Comment out <code>fhirServerUrl</code></li> <li>Set <code>dbConfig</code> to the DB connection config file, e.g., <code>docker/config/hapi-postgres-config_local.json</code>; </li> <li>Edit the values in this file to match the database for the FHIR server you are connecting to.</li> </ol>"},{"location":"tutorial_single_machine/#flattened-views","title":"Flattened views","text":"<p>With the default config, you will create both Parquet files (under <code>dwhRootPrefix</code>) and flattened views in the database configured by <code>sinkDbConfigPath</code> here.  * If you don't need flattened views you can comment out that setting.  * If you do need them, make sure you create the DB referenced in the connection config file, e.g., with the following SQL query:</p> <p><pre><code>CREATE DATABASE views;\n</code></pre> which you can run in Postgres like this: <pre><code>PGPASSWORD=admin psql -h 127.0.0.1 -p 5432 -U admin postgres -c \"CREATE DATABASE views\"\n</code></pre></p> <p>For documentation of all config parameters, see here.</p> <p>If you are using the local test servers, things should work with the default values. If not, use the IP address of the Docker default bridge network. To find it, run the following command and use the \"Gateway\" value:</p> <pre><code>docker network inspect bridge | grep Gateway\n</code></pre> <p>The Single Machine docker configuration uses two environment variables, <code>DWH_ROOT</code> and <code>PIPELINE_CONFIG</code>, whose default values are defined in the .env file. To override them, set the variable before running the <code>docker-compose</code> command. For example, to override the <code>DWH_ROOT</code> environment variable, run the following:</p> <pre><code>DWH_ROOT=\"$(pwd)/&lt;path_to_dwh_directory&gt;\" docker compose -f docker/compose-controller-spark-sql-single.yaml up --force-recreate \n</code></pre>"},{"location":"tutorial_single_machine/#run-the-single-machine-configuration","title":"Run the Single Machine configuration","text":"<p>To bring up the <code>docker/compose-controller-spark-sql-single.yaml</code> configuration for the first time or if you have run this container in the past and want to include new changes pulled into the repo, run:</p> <pre><code>docker compose -f docker/compose-controller-spark-sql-single.yaml up --force-recreate --build\n</code></pre> <p>Alternatively, to run without rebuilding use:</p> <pre><code>docker compose -f docker/compose-controller-spark-sql-single.yaml up --force-recreate\n</code></pre> <p>Alternatively, <code>docker/compose-controller-spark-sql.yaml</code> serves as a very simple example on how to integrate the Parquet output of Pipelines in a Spark cluster environment.</p> <p>Once started, the Pipelines Controller is available at <code>http://localhost:8090</code> and the Spark Thrift server is at <code>http://localhost:10001</code>.</p> <p>The first time you run the Pipelines Controller, you must manually start a Full Pipeline run. In a browser go to <code>http://localhost:8090</code> and click the Run Full button. </p> <p>After running the Full Pipeline, use the Incremental Pipeline to update the Parquet files and tables. By default it is scheduled to run every hour, or you can manually trigger it.</p> <p>If the Incremental Pipeline does not work, or you see errors like:</p> <pre><code>ERROR o.openmrs.analytics.PipelineManager o.openmrs.analytics.PipelineManager$PipelineThread.run:343 - exception while running pipeline: \npipeline-controller    | java.sql.SQLException: org.apache.hive.service.cli.HiveSQLException: Error running query: org.apache.spark.sql.AnalysisException: Unable to infer schema for Parquet. It must be specified manually.\n</code></pre> <p>try running <code>sudo chmod -R 755</code> on the Parquet file directory, by default located at <code>docker/dwh</code>.</p>"},{"location":"tutorial_single_machine/#view-and-analyze-the-data-using-spark-thrift-server","title":"View and analyze the data using Spark Thrift server","text":"<p>Connect to the Spark Thrift server using a client that supports Apache Hive. For example, if using the JDBC driver, the URL should be <code>jdbc:hive2://localhost:10001</code>. The pipeline will automatically create <code>Patient</code>, <code>Encounter</code>, and <code>Observation</code> tables when run.</p> <p>Let's do some basic quality checks to make sure the data is uploaded properly (note table names are case insensitive):</p> <p><pre><code>SELECT COUNT(0) FROM Patient;\n</code></pre> We should have exactly 79 patients: <pre><code>+-----------+\n| count(0)  |\n+-----------+\n| 79        |\n+-----------+\n</code></pre></p> <p>Doing the same for observations: <pre><code>SELECT COUNT(0) FROM Observation;\n</code></pre> <pre><code>+-----------+\n| count(0)  |\n+-----------+\n| 17279     |\n+-----------+\n</code></pre></p>"},{"location":"tutorial_single_machine/#whats-next","title":"What's next","text":"<ul> <li>Querying exported parquet files using SQL</li> <li>Building a dashboard with Apache SuperSet and FHIR Data Pipes</li> </ul>"},{"location":"tutorial_test_servers/","title":"Local Test Servers","text":"<p>This guide shows how to use provided Docker images to bring up test servers (and optionally load with synthetic data) to easily get started the FHIR Data Pipes pipelines. </p> <p>There are 3 Docker server configurations you can use for testing:</p> <ul> <li>HAPI FHIR server with Postgres (source)</li> <li>OpenMRS Reference Application with MySQL (source)</li> <li>HAPI FHIR server (destination)</li> </ul>"},{"location":"tutorial_test_servers/#instructions","title":"Instructions","text":"<p>Note: All commands are run from the root directory of the repository.</p> <ol> <li> <p>Create an external Docker network named <code>cloudbuild</code>:</p> <pre><code>docker network create cloudbuild\n</code></pre> </li> <li> <p>Bring up a FHIR source server for the pipeline. This can be either:</p> <ul> <li> <p>HAPI FHIR server with     Postgres:</p> <pre><code>docker-compose  -f ./docker/hapi-compose.yml up  --force-recreate -d\n</code></pre> <p>The base FHIR URL for this server is <code>http://localhost:8091/fhir</code>. If you get a CORS error when accessing the URL, try manually refreshing (e.g. ctrl-shift-r).</p> </li> <li> <p>OpenMRS Reference Application with     MySQL:</p> <pre><code>docker-compose -f ./docker/openmrs-compose.yml up --force-recreate -d\n</code></pre> <p>The base FHIR URL for this server is <code>http://localhost:8099/openmrs/ws/fhir2/R4</code></p> </li> </ul> </li> <li> <p>Upload the synthetic data stored in     sample_data     to the FHIR server that you brought up using the Synthea data     uploader.</p> <p>The uploader requires the <code>google-auth</code> Python library, which you can install using:</p> <pre><code>pip3 install --upgrade google-auth\n</code></pre> <p>For example, to upload to the HAPI FHIR server brought up in the previous step, run:</p> <pre><code>python3 ./synthea-hiv/uploader/main.py HAPI http://localhost:8091/fhir \\\n--input_dir ./synthea-hiv/sample_data --cores 8\n</code></pre> <p>Depending on your machine, using too many cores may slow down your machine or cause JDBC connection pool errors with the HAPI FHIR server. Reducing the number of cores using the <code>--cores</code> flag should help at the cost of increasing the time to upload the data.</p> </li> <li> <p>(optional) If you only want to output Apache Parquet files, there is no additional     setup. If you want to test outputting to another FHIR server, then bring up a     destination HAPI FHIR     server:</p> <pre><code>docker-compose  -f ./docker/sink-compose.yml up  --force-recreate -d\n</code></pre> <p>The base URL for this server is <code>http://localhost:8098/fhir</code>.</p> </li> </ol>"},{"location":"tutorial_test_servers/#additional-notes-for-openmrs","title":"Additional notes for OpenMRS","text":"<p>Once running you can access OpenMRS at http://localhost:8099/openmrs/ using username \"admin\" and password \"Admin123\". The Docker image includes the required FHIR2 module and demo data. Edit <code>docker/openmrs-compose.yaml</code> to change the default port.</p> <p>Note: If <code>docker-compose</code> fails, you may need to adjust file permissions. In particular if the permissions on <code>mysqld.cnf</code> is not right, the <code>datadir</code> set in this file will not be read by MySQL and it will cause OpenMRS to require its <code>initialsetup</code> (which is not needed since the MySQL image already has all the data and tables needed):</p> <pre><code>$ docker-compose -f docker/openmrs-compose.yaml down -v\n$ chmod a+r docker/mysql-build/mysqld.cnf\n$ chmod -R a+r ./utils\n$ docker-compose -f docker/openmrs-compose.yaml up\n</code></pre> <p>In order to see the demo data in OpenMRS you must rebuild the search index. In OpenMRS go to Home &gt; System Administration &gt; Advanced Administration. Under Maintenance go to Search Index then Rebuild Search Index.</p>"},{"location":"tutorials/","title":"Tutorials","text":"<p>In this section you can access different tutorials and colabs for working with FHIR Data Pipes.</p>"}]}